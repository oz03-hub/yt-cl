- We get more visible distinct clusters when less than 100 and greater, when mixed things get more ambiguous
- There are some clear outliers in the data, we can try to remove them, see how much they deviate from the general clump of points
- You can see clustering in PCA more clearly if you set limits on the axes
- We can see anomalies in transcript by looking at the normalized transcripts and calculating the lexical diversity
- Very low diversity, 0.05, show repetitive content, which might be from music videos, confusing the model
- bow might not be the best representation for this data, we can try to use tf-idf
- Clusters on graph look much more promising for embedding based clusterings
- inertia performs better when two corpuses are clustered individually, clustering all together results in a higher inertia. Inertia almost doubles, this might also be because we consider twice as many data points
- silhouette score is low for all corporas, we need a way to separate the clusters more, it is average performance right now, slightly better for less than 100 words corpus
- We could try to remove more common words and start incorporating document length normalization
- A higher value of CH indicates a better clustering, because it means that the data points are more spread out between clusters than they are within clusters.
- I think we need to take another look at preprocessing to prioritize more context bearing tokens
- We can also try applying hierarchical clustering, LSA
- silhouette, how well clusters are separated, better defined clusters
- davies, similarity between clusters
- calinski, well separated, dense clusters
- create document embeddings by weighting each word embedding with its tf-idf value and normalize by document length
- how can we handle out of vocabulary words? we can train fasttext on the corpus
- ensemble, find different embeddings through different techniques, average to make a decision

- Very clear clusters are about mechanic diy / car review, religion, rap a lot of profanities, cooking/baking, gaming
- A good cluster amount seems in range 10, 20. Even getting closer to 20 topics start to degrade
- Removing stopwords after training is just as effective as removing them before.
- Removing stopwords simply reduce the amount of probability mass and smoothing of the model caused by frequent non-topic terms
- Corpus specific stoplists provide little utility, it is sufficient to remove obvious stopwords
- post-hoc stopword removal can significantly improve coherence

- post-hoc removal helped to identify topics easier, I notice with lower number of clusters, 10-15 some topics are still visible, car, religion, rap, gaming. Increasing cluster size creates more sub-topics, seems like gaming gets split more, and we see more emerging topics like chemistry, math, biology in 30 clusters, but also more non-sense generic classes

- I think post-hoc custom stop removal is very beneficial
- Some clusters do have significantly more lengthy documents
- bag of words actually performs really bad, must cluster have only 1 document
- tfidf has a better distribution of document numbers per cluster and document lengths, but some clusters have noticeably longer documents

- bow cannot generate coherent topics, most cluster sizes are really small and unreliable. Also most clear topics from tfidf is missing, car-review, radio news, math/chemistry hw

- Topic models for online content: Online LDA, Online HDP
- PMI for topic coherence, diversity of topics
- A key understanding is that, we don't have any prior knowledge of topic distributions on youtube, we have some metadata collected but can that sample tell use reliably about the population? Assuming not, we want to find many high coherence topics that would give us the highest confidence.

- embedding model and tfidf model show similar median doc length distributions. Noticed some clusters have <10 mean length. We could filter out documents in the below <5% doc length

- Qualitative evaluation is very valuable to have a human insight in the process and we should keep it throughout the process. However at this time it could be a one-man or llm operation, quite expensive but good heuristic to check-in with when we are satisfied with other scores and want to investigate more

- Online LDA has relied on PMI to evaluate its coherence and qualitative, no measure for coverage, like our case
- Online HDP has relied on LL for coverage, no coherence, but qualitative analysis. Contradictory to our situation
(HDP) hierarchical dirichlet process: non-parametric adaptation of lda that approximates k using probabilistic model

- LDA: documents are not generated according to only one topic. allow a document to be generated by a distribution of topics. Goal to maximize the likelihood of documents over k topics. alpha: topics-per-doc ratio, higher more topic per doc. Beta: words-per-topic, lower fewer words per topic

- online lda, runs lda on small batches, sliding window format. 

- 18 clusters yield highest pmi for tfidf. There is no linear relation
- a good repo about a collection of diversity measures: https://github.com/silviatti/topic-model-diversity/blob/master/diversity_metrics.py
- proportion of unique words performs best between 10-22, non-linear, for tfidf

- do same clustering with random seeds, check assignment cluster stability, variation of information, mutual information: pulling out stops
- find central videos for each cluster, 5 videos, create labeling task, only 18 clusters
- tfidf, embedding, lda for each
- comparing clusterings by variation of information
- look at sent papers from virginia
- do stability first, lda

- classification with key terms, metric of quality for representative terms, from pulling out all stops
- NPMI to check if coherence changes across different clustering runs, from pulling out all stops
- Can mutual information be used to check stability? Looks like variation of information is closely related to MI, but it is a true metric.

- got mean VI 2.29 for 18 clusters, 10 different runs

- vi 12:1.84, 13:1.82, 14:1.92, 15:2.01, 16:1.98, 17:2.16, 18:2.24, 19:2.23, 20:2.37
- stemmer paper show that light stemming may improve comparative similarity slightly

- check again youtube music database and drop
- try word2vec first experiment, if performance does not increase look into videos with useful english, filter by number of words maybe

- used openai llms to categorize transcripts into good/bad
- used signal processing to categorize into speech/other
- using voice activity detection
